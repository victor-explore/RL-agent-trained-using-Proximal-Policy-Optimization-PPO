# RL agent trained using PPO
## Overview
This project demonstrates a Reinforcement Learning (RL) environment built using Unity and the ML-Agents toolkit.
The goal is to train an intelligent agent, named "Cubie," to navigate a simple 2D environment and reach a designated "Hiding Spot" while avoiding collisions with "Walls."
This environment is designed to be a foundational step towards developing more complex hiding and seeking behaviors for agents in simulated environments.

## Reinforcement Learning Methodology: Proximal Policy Optimization (PPO)
This project employs Proximal Policy Optimization (PPO), a state-of-the-art policy gradient method in reinforcement learning.
PPO is chosen for its stability and sample efficiency, making it well-suited for training agents in complex environments.
It iteratively improves the agent's policy by taking steps in the policy space that are guaranteed not to be too far from the previous policy, ensuring stable learning.
This is achieved through a clipped surrogate objective function that prevents overly large policy updates, which can lead to performance degradation.
PPO's strength lies in its ability to balance exploration and exploitation effectively.
By limiting the change in policy at each update, PPO avoids drastic shifts that can destabilize training.
Furthermore, PPO is designed to be relatively easy to implement and tune compared to other policy gradient methods, making it a practical choice for a wide range of RL tasks.
The algorithm's on-policy nature means it learns directly from the experiences generated by its current policy, which contributes to its sample efficiency.
In essence, PPO provides a robust and efficient framework for training agents to learn complex behaviors through trial and error in simulated environments.
The training progress can be visualized in the following graph, which shows the reward and reward standard deviation over time:
![Training Graph](https://github.com/victor-explore/RL-agent-trained-using-Proximal-Policy-Optimization-PPO/blob/059bb476689e4856e0ccdd48b2a5c2aa2076b969/output.png)
[Training Graph Source](https://github.com/victor-explore/RL-agent-trained-using-Proximal-Policy-Optimization-PPO/blob/059bb476689e4856e0ccdd48b2a5c2aa2076b969/output.png)

## Environment Description
The Unity environment is minimalistic, comprising the following key elements:
- **Agent ("Cubie"):** The learning entity, tasked with reaching the "Hiding Spot."
  - The agent is equipped with a `Box Collider` for collision detection and a `Rigidbody` for physics interactions, although gravity is disabled and it is not kinematic to allow for direct control via actions.
  - The `AgentController.cs` script governs the agent's behavior, implementing the RL logic.
- **Target ("Hiding Spot"):** The goal location for the agent.
  - Reaching the "Hiding Spot" yields a positive reward, encouraging the agent to seek it out.
  - It is tagged as "Hiding Spot" for easy identification in the agent's script.
- **Walls ("Wall"):** Obstacles that the agent must avoid.
  - Colliding with a "Wall" results in a negative reward, penalizing undesirable behavior.
  - All wall objects are tagged as "Wall."
- **Floor ("Floor"):** The ground surface on which the agent and target move.
  - Provides a bounded 2D plane for the agent to operate within.
The environment is deliberately kept simple to focus on the core RL learning process.
The 2D nature simplifies the movement and observation space, allowing for faster training and easier analysis of the agent's learned behavior.
The placement of the target is randomized at the beginning of each episode to ensure the agent learns a general policy for reaching the hiding spot regardless of its initial position relative to the agent.
The walls are positioned to constrain the agent's movement and introduce the challenge of obstacle avoidance, which is a fundamental aspect of navigation tasks.
To visualize the environment, please refer to the following image:
![Environment Image](https://github.com/victor-explore/RL-agent-trained-using-Proximal-Policy-Optimization-PPO/blob/059bb476689e4856e0ccdd48b2a5c2aa2076b969/1111.PNG)
[Environment Image Source](https://github.com/victor-explore/RL-agent-trained-using-Proximal-Policy-Optimization-PPO/blob/059bb476689e4856e0ccdd48b2a5c2aa2076b969/1111.PNG)

## Observation Space
The agent's observation space is designed to provide it with essential information about its surroundings.
It is a continuous vector of size 6, consisting of:
- **Agent's Local Position (Vector3):**  The agent's (x, y, z) coordinates relative to the environment's origin.
  - This is obtained using `transform.localPosition` in the `CollectObservations` function of `AgentController.cs`.
  - The y-coordinate is essentially constant (0.0196f) in this 2D environment, but is included as part of the Vector3.
- **Target's Local Position (Vector3):** The target's (x, y, z) coordinates relative to the environment's origin.
  - This is obtained using `target.localPosition` in the `CollectObservations` function of `AgentController.cs`.
  - Similar to the agent's position, the y-coordinate of the target is also constant (0f).

This observation space allows the agent to perceive its own location and the location of the target, enabling it to learn to navigate towards the target.
The choice of local positions is crucial as it provides the agent with a coordinate system relative to the environment's origin, allowing it to understand its spatial relationship with the target and the environment boundaries.
Using Vector3 for positions, even in a primarily 2D environment, maintains consistency with Unity's coordinate system and allows for potential future expansion to 3D environments without significant changes to the observation structure.
The size of the observation space (6) is minimal, ensuring efficient processing and learning, while still providing the necessary information for the agent to perform the task.

## Action Space
The agent's action space is designed for simple linear movement along the x-axis.
It is a continuous action space with a single dimension:
- **Continuous Action (Move):** A single float value representing the agent's desired movement along the x-axis.
  - This value is received in the `OnActionReceived` function of `AgentController.cs` from the neural network.
  - The action value is typically in the range of [-1, 1], where -1 corresponds to maximum leftward movement and 1 corresponds to maximum rightward movement.
  - A `moveSpeed` multiplier (set to 2f) scales this action to control the agent's velocity.
  - The actual movement is implemented using `transform.localPosition += new Vector3(move, 0f) * Time.deltaTime * moveSpeed;`, ensuring frame-rate independent movement.

This action space allows the agent to control its horizontal movement, which is sufficient for reaching the target in this 1D movement task.
Restricting the action space to a single continuous action simplifies the learning problem and focuses the agent's learning on mastering linear navigation.
The continuous nature of the action space allows for fine-grained control of the agent's movement, enabling smoother and more efficient trajectories towards the target compared to discrete action spaces.
The use of `Time.deltaTime` in the movement update is essential for ensuring that the agent's movement speed is consistent across different frame rates, making the training process more robust and transferable to different hardware.
The `moveSpeed` parameter provides a way to tune the agent's responsiveness to actions, allowing for adjustments to the agent's agility and control.

## Reward Function
The reward function is crucial for guiding the agent's learning process.
It is designed to incentivize reaching the "Hiding Spot" and penalize collisions with "Walls."
- **Reaching "Hiding Spot":**  A reward of +10.0f is given when the agent's collider enters the trigger area of the "Hiding Spot."
  - Implemented in the `OnTriggerEnter` function in `AgentController.cs` when `other.gameObject.CompareTag("Hiding Spot")` is true.
  - This encourages the agent to move towards and reach the designated target area.
- **Colliding with "Wall":** A penalty of -5.0f is given when the agent's collider enters the trigger area of a "Wall."
  - Implemented in the `OnTriggerEnter` function in `AgentController.cs` when `other.gameObject.CompareTag("Wall")` is true.
  - This discourages the agent from colliding with obstacles and promotes navigation within the allowed space.

The episode ends immediately after receiving either a positive or negative reward using `EndEpisode()`, ensuring that each episode is a discrete attempt to reach the target without hitting walls.
The magnitude of the rewards and penalties is carefully chosen to create a clear signal for the agent.
The positive reward for reaching the hiding spot is significantly larger than the negative penalty for hitting a wall, encouraging the agent to prioritize reaching the target while still learning to avoid obstacles.
Ending the episode upon reaching the target or hitting a wall creates distinct learning episodes, allowing the agent to reset and attempt the task again from a new starting configuration.
This episodic structure is common in reinforcement learning and facilitates learning by breaking down the task into manageable segments.
The trigger-based reward system, using `OnTriggerEnter`, is efficient for detecting when the agent reaches the target or collides with a wall, providing immediate feedback to the agent and driving the learning process.

## Heuristic Function
For debugging and manual control, the `Heuristic` function is implemented in `AgentController.cs`.
- **Manual Control:**  Allows a human user to control the agent's horizontal movement using keyboard input (left/right arrow keys or A/D keys).
  - `Input.GetAxis("Horizontal")` is used to get input values in the range [-1, 1].
  - This input is directly mapped to the agent's continuous action, overriding the neural network's output when the behavior type is set to "Heuristic."
  - Useful for verifying environment setup and agent behavior before and during training.

The heuristic function serves as a valuable tool for development and analysis.
By allowing manual control, it enables developers to test the environment setup, verify the agent's movement mechanics, and gain an intuitive understanding of the task.
It can also be used to collect demonstration data, which can be helpful for techniques like imitation learning or for initializing the agent's policy before starting reinforcement learning.
The `Input.GetAxis("Horizontal")` function provides a convenient way to map keyboard input to continuous values, making it easy to control the agent's horizontal movement smoothly.
The heuristic function is particularly useful in the early stages of development and during debugging, ensuring that the basic environment and agent interactions are working as expected before engaging in automated training.

## Setup Instructions
To set up this project, follow these steps:
1. **Python Environment:** Create a conda environment named "ml-agents" with Python 3.9.13.
   ```bash
   conda create -n ml-agents python=3.9.13
   conda activate ml-agents
   ```
2. **Install Python Packages:** Install the necessary Python packages using pip.
   ```bash
   pip install mlagents protobuf onnx torch
   ```
3. **Unity Project:** Ensure you have Unity installed and open the project in Unity.
  - Import the ML-Agents Unity package version 2.0.1 into your Unity project.
  - Verify that the project hierarchy is set up as described in `Documentation.md`.

Setting up the Python environment with conda ensures a consistent and isolated environment for running the ML-Agents toolkit and managing dependencies.
Specifying Python 3.9.13 ensures compatibility with the ML-Agents version used in this project.
Installing the required Python packages, including `mlagents`, `protobuf`, `onnx`, and `torch`, provides the necessary libraries for training and exporting the agent's neural network model.
Importing the correct version of the ML-Agents Unity package (2.0.1) is crucial for compatibility between the Unity environment and the Python training scripts.
Verifying the project hierarchy ensures that all the necessary game objects and components are correctly set up in the Unity scene, as described in the project documentation.

## Usage
1. **Training:** To train the agent, use the `mlagents-learn` command from your conda environment, pointing to the configuration file and run settings.
   ```bash
   mlagents-learn config/ppo/HidingAgent.yaml --run-id=HidingAgentRun
   ```
  - Replace `config/ppo/HidingAgent.yaml` with the path to your training configuration file.
  - `--run-id=HidingAgentRun` sets a unique identifier for the training run.
2. **Running Inference:** After training, copy the trained model (.onnx file) to the `Model` field in the `Behavior Parameters` component of the "Cubie" agent in Unity.
  - Set the `Behavior Type` in `Behavior Parameters` to "Inference" to use the trained model.
  - Run the Unity scene to observe the trained agent navigating to the "Hiding Spot."

The `mlagents-learn` command is the primary entry point for training agents using the ML-Agents toolkit.
The configuration file (`HidingAgent.yaml` in this example) specifies the training parameters, network architecture, and environment settings.
The `--run-id` parameter allows for organizing and tracking different training runs, which is essential for experimentation and hyperparameter tuning.
After training, the trained model is exported as an `.onnx` file, which can be imported into Unity to control the agent's behavior during inference.
Setting the `Behavior Type` to "Inference" in the `Behavior Parameters` component tells the Unity agent to use the trained model instead of learning online or using a heuristic.
Running the Unity scene after importing the trained model allows for visualizing and evaluating the agent's learned behavior in the environment.

## File Descriptions
- **`Documentation.md`:** This file provides a comprehensive overview of the project, including the environment setup, agent components, and project goals.
  - It serves as the primary documentation for understanding the project structure and design.
- **`AgentController.cs`:** This C# script is attached to the "Cubie" agent in Unity and contains the core RL logic.
  - It implements the `Agent` class from ML-Agents, defining the observation collection, action processing, reward mechanism, and heuristic control.
  - It is responsible for the agent's interaction with the Unity environment and its learning process.

The `Documentation.md` file acts as the central repository of information about the project, providing a detailed guide for users and developers.
It includes explanations of the environment design, RL methodology, observation and action spaces, reward function, setup instructions, and usage guidelines.
The `AgentController.cs` script is the core component of the agent's behavior, implementing the RL algorithms and defining how the agent interacts with the Unity environment.
This script is responsible for collecting observations from the environment, processing actions from the neural network, applying rewards and penalties, and implementing the heuristic control for manual operation.
Together, these files provide a complete and well-documented framework for training and deploying a hiding agent in a simulated environment using the ML-Agents toolkit and Proximal Policy Optimization.
